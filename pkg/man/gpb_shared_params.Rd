% Generated by roxygen2: do not edit by hand
% Please edit documentation in R/shared_pars_others.R
\name{gpb_shared_params}
\alias{gpb_shared_params}
\title{Shared parameter docs}
\arguments{
\item{callbacks}{list of callback functions
List of callback functions that are applied at each iteration.}

\item{data}{A \code{gpb.Dataset} object. Some functions, such as \code{gpboost} and \code{gpb.cv}, 
allow you to pass other types of data such as \code{matrix} and then separately 
supply \code{label} as argument.}

\item{early_stopping_rounds}{int
Activates early stopping.
Requires at least one validation data and one metric
If there's more than one, will check all of them except the training data
Returns the model with (best_iter + early_stopping_rounds)
If early stopping occurs, the model will have 'best_iter' field}

\item{eval_freq}{Evaluation output frequency, only effect when verbose > 0}

\item{init_model}{Path of model file of \code{gpb.Booster} object, will continue training from this model}

\item{nrounds}{Number of boosting iterations (= number of trees). This is the most important tuning parameter for boosting}

\item{params}{List of parameters (many of them tuning paramters), see Parameters.rst for more information. A few key parameters:
\itemize{
    \item{learning_rate}{ The learning rate, also called shrinkage or damping parameter 
    (default = 0.1). An important tuning parameter for boosting. Lower values usually 
    lead to higher predictive accuracy but more boosting iterations are needed }
    \item{num_leaves}{ Number of leaves in a tree. Tuning parameter for 
    tree-boosting (default = 127)}
    \item{min_data_in_leaf}{ Minimal number of samples per leaf. Tuning parameter for 
    tree-boosting (default = 20)}
    \item{max_depth}{ Maximal depth of a tree. Tuning parameter for tree-boosting (default = no limit)}
    \item{leaves_newton_update}{ Set this to TRUE to do a Newton update step for the tree leaves 
    after the gradient step. Applies only to Gaussian process boosting (GPBoost algorithm)}
    \item{train_gp_model_cov_pars}{ If TRUE, the covariance parameters of the Gaussian process 
    are stimated in every boosting iterations, 
    otherwise the GPModel parameters are not estimated. In the latter case, you need to 
    either esimate them beforehand or provide the values via 
    the 'init_cov_pars' parameter when creating the GPModel (default = TRUE).}
    \item{use_gp_model_for_validation}{ If TRUE, the Gaussian process is also used 
    (in addition to the tree model) for calculating predictions on the validation data 
    (default = FALSE)}
    \item{use_nesterov_acc}{ Set this to TRUE to do boosting with Nesterov acceleration. 
    Can currently only be used for tree_learner = "serial" (default option)}
    \item{nesterov_acc_rate}{ Acceleration rate for momentum step in case Nesterov accelerated 
    boosting is used (default = 0.5)}
    \item{boosting}{ Boosting type. \code{"gbdt"} or \code{"dart"}. Only "gpdt" allows for 
    doing Gaussian process boosting}
    \item{num_threads}{ Number of threads. For the best speed, set this to
                       the number of real CPU cores, not the number of threads (most
                       CPU using hyper-threading to generate 2 threads per CPU core).}
}}

\item{verbose}{Verbosity for output, if <= 0, also will disable the print of evaluation during training}

\item{label}{Vector of response variables / labels, used if \code{data} is not an \code{\link{gpb.Dataset}}}

\item{weight}{Vector of weights for samples (default = NULL). This is currently not supported for Gaussian process boosting (i.e. it only affects the trees and the Gaussian process or random effects model ignores the weights)}

\item{reset_data}{Boolean, setting it to TRUE (not the default value) will transform the booster model into a predictor model which frees up memory and the original datasets}

\item{valids}{A list of \code{gpb.Dataset} objects, used as validation data}

\item{obj}{Objective function, can be character or custom objective function (default = "regression_l2"). Examples include
\code{regression_l2}, \code{regression_l1}, \code{huber},
\code{binary}, \code{lambdarank}, \code{multiclass}, \code{multiclass}. Currently only "regression_l2" supports Gaussian process boosting.}

\item{eval}{Evaluation function, can be (a list of) character or custom eval function}

\item{record}{Boolean, TRUE will record iteration message to \code{booster$record_evals}}

\item{colnames}{Feature (covariate) names, if not null, will use this to overwrite the names in dataset}

\item{categorical_feature}{List of str or int
type int represents index,
type str represents feature names}

\item{callbacks}{list of callback functions
List of callback functions that are applied at each iteration.}

\item{gp_model}{A \code{GPModel} object that contains the random effects (Gaussian process and / or grouped random effects) model. Can currently only be used for objective = "regression"}

\item{use_gp_model_for_validation}{Boolean (default = FALSE). If TRUE, the Gaussian process is also used (in addition to the tree model) for calculating predictions on the validation data}

\item{train_gp_model_cov_pars}{Boolean (default = TRUE). If TRUE, the covariance parameters of the Gaussian process are estimated in every boosting iterations, 
otherwise the GPModel parameters are not estimated. In the latter case, you need to either esimate them beforehand or provide the values via 
the 'init_cov_pars' parameter when creating the GPModel}
}
\description{
Parameter docs shared by \code{gpb.train}, \code{gpb.cv}, and \code{gpboost}
}
